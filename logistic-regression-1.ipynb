{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10160592,"sourceType":"datasetVersion","datasetId":6274020},{"sourceId":10160597,"sourceType":"datasetVersion","datasetId":6274024}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntrain_data = pd.read_csv(\"/kaggle/input/train-1/Gotem Pumpkins.csv\", index_col=0)\ntest_data = pd.read_csv(\"/kaggle/input/test-1/Freyja_Pumpkins.csv\", index_col=0)\n\nclass_labels = train_data['Class'].unique()\nclass_mapping = {label: idx for idx, label in enumerate(class_labels)}\n\ntrain_data['Class'] = train_data['Class'].map(class_mapping)\ntest_data['Class'] = test_data['Class'].map(class_mapping)\n\n#making sure feature columns are numeric\ntrain_data = train_data.apply(pd.to_numeric, errors='coerce')\ntest_data = test_data.apply(pd.to_numeric, errors='coerce')\n\n#handle missing values (fill with 0)\ntrain_data = train_data.fillna(0)\ntest_data = test_data.fillna(0)\n\n#separate features and target variable for training\ny_train = train_data['Class'].values  # target variable\nx_data = train_data.drop(['Class'], axis=1)  # features\ntrain_mean = np.mean(x_data, axis=0)\ntrain_std = np.std(x_data, axis=0)\n\nx_train = (x_data - train_mean) / train_std \nx_test_data = test_data.drop(['Class'], axis=1)\nx_test = (x_test_data - train_mean) / train_std  # using training data's mean and std to standardize test data\n\n#Reshape target variables for logistic regression\ny_train = y_train.reshape(1, -1)\ny_test = test_data['Class'].values.reshape(1, -1)\n\n#logistic Regression functions\ndef initialize_weights_and_bias(dimension):\n    w = np.zeros((dimension, 1))  # Initialize weights to zero\n    b = 0.0  # Initialize bias to zero\n    return w, b\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef forward_backward_propagation(w, b, x_train, y_train):\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train * np.log(y_head) - (1 - y_train) * np.log(1 - y_head)\n    cost = np.sum(loss) / x_train.shape[1]\n\n    #backward propagation\n    derivative_weight = np.dot(x_train, (y_head - y_train).T) / x_train.shape[1]\n    derivative_bias = np.sum(y_head - y_train) / x_train.shape[1]\n\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    return cost, gradients\n\ndef update(w, b, x_train, y_train, learning_rate, num_iterations):\n    cost_list = []\n    for i in range(num_iterations):\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        \n        #update weights and bias\n        w -= learning_rate * gradients[\"derivative_weight\"]\n        b -= learning_rate * gradients[\"derivative_bias\"]\n        \n        if i % 10 == 0:\n            print(f\"Cost after iteration {i}: {cost}\")\n\n    plt.plot(range(0, num_iterations, 10), cost_list[::10])\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Cost over Iterations\")\n    plt.show()\n\n    return w, b, cost_list\n\ndef predict(w, b, x_test):\n    z = sigmoid(np.dot(w.T, x_test) + b)\n    y_prediction = (z > 0.5).astype(int)\n    return y_prediction\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    dimension = x_train.shape[0]\n    w, b = initialize_weights_and_bias(dimension)\n    \n    # Update weights and bias\n    w, b, cost_list = update(w, b, x_train, y_train, learning_rate, num_iterations)\n   \n    y_prediction_test = predict(w, b, x_test)\n   \n    accuracy = 100 - np.mean(np.abs(y_prediction_test - y_test)) * 100\n    print(f\"Test accuracy: {accuracy} %\")\n\nlogistic_regression(x_train.T, y_train, x_test.T, y_test, learning_rate=0.01, num_iterations=200)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}